<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <meta name="googlebot" content="noindex, nofollow" />
        <meta name="Googlebot-Mobile" content="noindex, nofollow" />
        <meta name="Baiduspider" content="noindex, nofollow" />
        <meta name="Sogou web spider" content="noindex, nofollow" />

        <title>gyk-notes - SVM学渣笔记</title>
        <link href="http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700" rel="stylesheet" type="text/css" />
        <link rel="stylesheet" type="text/css" href="../../../css/default.css" />
        <link rel="stylesheet" type="text/css" href="../../../css/syntax.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../../../">gyk-notes</a>
            </div>
            <div id="navigation">
                <a href="../../../">Home</a>
                <a href="../../../about.html">About</a>
                <a href="../../../contact.html">Contact</a>
                <a href="../../../archive.html">Archive</a>
            </div>
        </div>
        <div class="sep-line"></div>
        <div id="content">
            <h1>SVM学渣笔记</h1>
            <div class="info">
    Posted on January  7, 2016
    
</div>
<div class="info">
    
    Tags: <a title="All pages tagged 'MachineLearning'." href="../../../tags/MachineLearning.html">MachineLearning</a>
    
</div>

<p><strong>警告</strong>：本文包含大量公式，MathJax渲染时会导致CPU显著发热。当您阅读这些公式的时候，您的大脑也会发热。为避免过热造成伤害，请在阴凉环境下阅读本文。</p>
<h2 id="导言">导言</h2>
<p>SVM（Support Vector Machine，支持向量机）是一种常用的机器学习算法。为什么SVM使用如此广泛？因为它是为数不多的理论与实践俱佳的算法。神经网络在数据集上刷成绩是一把好手，但是调参基本属于黑魔法，让人感觉知其然而不知其所以然。另有一些模型尽管理论形式优美，一旦要解决实际问题就中看不中用。而SVM既有比较严谨的理论支撑，实战中应用起来也得心应手，自然受到了学术界和工业界的追捧。</p>
<p>课本上讲SVM的章节经常假设你已经了解拉格朗日对偶、KKT条件、VC维，可事实上当我之前开始学习SVM的时候，连什么是凸函数都想不起来了。因此本文把各种支离破碎的概念都整合了进来，尽量从直觉出发，不追求推导严谨，由学渣而写，为学渣而写。</p>
<h2 id="问题描述">问题描述</h2>
<p>对于一个二分类问题，</p>
<p><span class="math display">\[
\mathcal{D} = \left\{ (\mathbf{x}_i, y_i)\mid\mathbf{x}_i \in \mathbb{R}^d,\,
y_i \in \{-1,1\}\right\}_{i=1}^n
\]</span></p>
<p>我们定义以下形式的线性模型：</p>
<p><span class="math display">\[
y(\mathbf{x}) = \mathbf{w}\cdot\mathbf{x}-b \tag{1}
\]</span></p>
<p>通过映射 <span class="math inline">\(y(\mathbf{x}) \geq 0 \to 1,\ y(\mathbf{x})&lt;0 \to -1\)</span> ，得到一个线性分类器。</p>
<p>从直观上理解，我们想找到一个 <span class="math inline">\(d\)</span> 维超平面把类别标签 <span class="math inline">\(y\)</span> 等于1和-1的样本分开（暂时假设 <span class="math inline">\(\mathcal{D}\)</span> 确实线性可分）。根据解析几何，<span class="math inline">\(y = \mathbf{w}\cdot\mathbf{x}-b\)</span>定义了一个 <span class="math inline">\(d+1\)</span> 维的超平面，它与 <span class="math inline">\(d+1\)</span> 维超平面 <span class="math inline">\(y = 0\)</span> 相交于 <span class="math inline">\(d\)</span> 维超平面 <span class="math inline">\(\mathbf{w}\cdot\mathbf{x}-b=0\)</span>。</p>
<p>如果存在许多可行的分类方案，究竟哪一个是最优的？显然超平面的最优位置必须居中，即它到正样本的最小距离与负样本的最小距离一样。然后，这个“最小距离”必须尽可能的大。这需要引入一个“最大间隔分类器”的概念。下面首先给出“间隔”的定义。</p>
<p><strong>函数间隔（functional margin）与几何间隔（geometric margin）</strong>：对于训练集的第 i 个样本 <span class="math inline">\((\mathbf{x}_i, y_i)\)</span> ，定义 <em>函数间隔</em> 为</p>
<p><span class="math display">\[
\hat{\gamma}_i = y_i(\mathbf{w}\cdot\mathbf{x}_i-b)
\]</span></p>
<p>对于训练集<span class="math inline">\(\mathcal{D}\)</span>，定义 <em>函数间隔</em> 为</p>
<p><span class="math display">\[
\hat{\gamma} = \min_{i=1,\ldots,n} y_i(\mathbf{w}\cdot\mathbf{x}_i-b)
\]</span></p>
<p>有些机器学习算法比如 Logistic regression 会把线性变换后的值映射到概率，这时候函数间隔越大，就说明分类越清晰，可靠性越高。但对于 SVM 来说，线性变换后接入的是一个类似 <span class="math inline">\(\text{sgn}(\cdot)\)</span> 的函数，分类结果只和符号相关。可以发现，在 <span class="math inline">\(\text{Eq. }(1)\)</span> 中 <span class="math inline">\(\mathbf{w}\)</span> 和 <span class="math inline">\(b\)</span> 同时乘上一个缩放因子并不影响 <span class="math inline">\(y(\mathbf{x})=0\)</span> 的解，因此不妨把它们依照 <span class="math inline">\(\|\mathbf{w}\|\)</span> 正则化，即定义 <em>几何间隔</em> 为任意样本到分类边界的最小距离，表示为 <span class="math inline">\(\gamma\)</span> 。</p>
<p>容易知道 <span class="math inline">\(\mathbf{w}\)</span> 是 <span class="math inline">\(\mathbf{w}\cdot\mathbf{x}-b=0\)</span> 的法线，这是因为对于 <span class="math inline">\(d\)</span> 维分隔超平面上的两个点 <span class="math inline">\(\mathbf{x}_0,\mathbf{x}_1\)</span> ，有</p>
<p><span class="math display">\[
\mathbf{w}\mathbf{x}_0-b=\mathbf{w}\mathbf{x}_1-b=0 \implies
\mathbf{w}(\mathbf{x}_1 - \mathbf{x}_0)=0
\]</span></p>
<p>超平面的单位法线则是 <span class="math inline">\(\frac{\mathbf{w}}{\|\mathbf{w}\|}\)</span> 。</p>
<p>对于任意点 <span class="math inline">\(\mathbf{x}_i\)</span> ，取在超平面上的点 <span class="math inline">\(\mathbf{x}_0\)</span> ，向量 <span class="math inline">\(\overrightarrow{\mathbf{x}_0 \mathbf{x}_i}\)</span> 与单位法线的点积就是该点到超平面的距离：</p>
<p><span class="math display">\[
\gamma_i = \bigg| \frac{\mathbf{w}}{\|\mathbf{w}\|} \cdot (\mathbf{x}_i - \mathbf{x}_0) \bigg|
\]</span></p>
<p>代入线性变换，用标签 <span class="math inline">\(y_i\)</span> 去掉绝对值符号，可以得到：</p>
<p><span class="math display">\[
\gamma = \min_{i=1,\ldots,n} y_i\left(\frac{\mathbf{w}}{\|\mathbf{w}\|}\cdot
\mathbf{x}-\frac{b}{\|\mathbf{w}\|}\right)
\]</span></p>
<!-- 假如我们用的是神经网络，依初始权重和偏置取值的不同，将得到能正确分类训练集的不同模型，
然后可以用交叉验证的方法选出泛化（generalization）能力更好的那个。
所以当数据量比较小的时候， -->
<blockquote>
<p>如果用简写的方法， <span class="math inline">\(\hat{\mathbf{w}} = (-b,\mathbf{w}),\ \hat{\mathbf{x}} = (1,\mathbf{x})\)</span>，其中 <span class="math inline">\(\mathbf{x},\mathbf{w}\in\mathbb{R}^{d}\)</span> ，超平面方程也可以写作 <span class="math inline">\(\hat{\mathbf{w}} \cdot \hat{\mathbf{x}}=0\)</span> 。但是这里还是把偏置值显式写出。</p>
</blockquote>
<p>对于 <span class="math inline">\(\|\mathbf{w}\|\)</span> 的几何意义，可以理解成 <span class="math inline">\(d+1\)</span> 维超平面的“倾角正切值”，即functional margin和geometric margin相差的缩放因子。但是只要 <span class="math inline">\(d\)</span> 维的超平面（交线）位置不变，无论 <span class="math inline">\(\|\mathbf{w}\|\)</span> 和 <span class="math inline">\(b\)</span> 具体多大，对于分类结果是不会有影响的。</p>
<h3 id="优化函数">优化函数</h3>
<p>我们的目标是最大化几何间隔，</p>
<p><span class="math display">\[
\begin{aligned}
\text{maximize}\ \ &amp; \gamma \\
\text{s.t.}\ \ &amp; y_i(\mathbf{w}\cdot\mathbf{x}_i-b) \geq \gamma,\ \ i=1,\ldots,n \\
&amp; \|\mathbf{w}\|=1
\end{aligned}
\]</span></p>
<p>可惜上面的式子有一个问题： <span class="math inline">\(\|\mathbf{w}\|=1\)</span> 这个约束是非凸的，并没有现成的（凸）优化软件包可以直接调用。上面说过，在 <span class="math inline">\(\text{Eq. }(1)\)</span> 中 <span class="math inline">\(\mathbf{w}\)</span> 和 <span class="math inline">\(b\)</span> 同时乘上一个缩放因子并不影响结果，所以我们不妨设距离分类边界最近的正（负）样本代入 <span class="math inline">\(y(\mathbf{x})\)</span> 的值是1（-1）。因此，分隔超平面与 <span class="math inline">\(y = 1\)</span> 和 <span class="math inline">\(y = -1\)</span> 相交于边界超平面 <span class="math inline">\(\mathbf{w}\cdot\mathbf{x}-b=1\)</span> 和 <span class="math inline">\(\mathbf{w}\cdot\mathbf{x}-b=-1\)</span> 。</p>
<p>点 <span class="math inline">\(\mathbf{x}\)</span> 到超平面的距离为 <span class="math display">\[
\begin{aligned}
&amp; \frac{\mathbf{w}}{\left\| \mathbf{w} \right\|}
\cdot (\mathbf{x} - \mathbf{x}_0) \\
=&amp; \frac{\mathbf{w}\cdot \mathbf{x}-b}{\|\mathbf{w}\|} \\
=&amp; \frac{\hat{\mathbf{w}} \cdot \hat{\mathbf{x}}}{\|\mathbf{w}\|}
\end{aligned}
\]</span></p>
<p>所以原点到这个超平面的距离就是 <span class="math inline">\(\frac{b}{\|\mathbf{w}\|}\)</span> 。两个边界超平面 <span class="math inline">\(\mathbf{w}\cdot\mathbf{x}-b=1\)</span> 和 <span class="math inline">\(\mathbf{w}\cdot\mathbf{x}-b=-1\)</span> 的距离是 <span class="math inline">\(\frac{2}{\|\mathbf{w}\|}\)</span> 。</p>
<p>最大化 <span class="math inline">\(\|\mathbf{w}\|^{-1}\)</span> 相当于最小化 <span class="math inline">\(\|\mathbf{w}\|^2\)</span> 。我们把前面的优化式做一些变换，</p>
<p><span class="math display">\[
\begin{aligned}
\text{minimize}\ \ &amp; \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{s.t.}\ \ &amp; y_i(\mathbf{w}\cdot\mathbf{x}_i-b) \geq \gamma,\ \ i=1,\ldots,n \\
&amp; \|\mathbf{w}\|=1
\end{aligned}
\]</span></p>
<p>这种形式的目标函数存在现成的凸优化软件包可以拿来使用，但是，求解对偶形式会有一些额外的优点，至于这些优点是什么，将会在后面给出。</p>
<hr />
<p><strong>定理</strong>： <span class="math inline">\(d+1\)</span> 个或更少的点在 <span class="math inline">\(d\)</span> 维空间中是线性可分的，如果这些点没有共 <span class="math inline">\(d\)</span> 维超平面（not cohyperplanar）。</p>
<p><strong>简单证明</strong>：超平面存在与否是仿射不变的，不失一般性，平移原数据点，取 <span class="math inline">\(\mathbf{x}_0\)</span> 作为原点， <span class="math inline">\(\mathbf{x}_1, \ldots, \mathbf{x}_d\)</span> 作为标准基，则 <span class="math inline">\(\mathbf{x}_1=(1,0,\ldots,0),\mathbf{x}_2=(0,1,\ldots,0),\ldots\)</span> 。构造超平面 <span class="math inline">\(\mathbf{w}\mathbf{x}_0=0, \mathbf{w}=(y_1,y_2,\ldots,y_n)^T\)</span> ，则有 <span class="math inline">\(\mathbf{w}\mathbf{x}_i=y_i, i= 1,\ldots,d\)</span> 。也就是说，不同类数据点的位置向量和超平面法线的点积符号相异，所以这个超平面可以把 <span class="math inline">\(\mathbf{x}_1,\mathbf{x}_2,\ldots，\mathbf{x}_n\)</span> 分开。接下来构造超平面 <span class="math inline">\(\mathbf{w}\mathbf{x}_0=-\epsilon y_0\)</span> ，显然它可以分开包括 <span class="math inline">\(\mathbf{x}_0\)</span> 的所有数据点。</p>
<p>链接： <a href="http://metaoptimize.com/qa/questions/7148/how-can-it-be-proved-that-if-number-of-samples-are-less-than-d1-then-the-sample-set-is-linearly-separable">How can it be proved that if number of samples are less than d+1, then the sample set is linearly separable?</a></p>
<hr />
<h2 id="拉格朗日算子与kkt条件">拉格朗日算子与KKT条件</h2>
<h3 id="单个约束">单个约束</h3>
<p>用二维空间作为例子，对于</p>
<p><span class="math display">\[
\text{minimize}\ f(x,y) \\
\text{s.t.}\ g(x,y)=c
\]</span></p>
<p><span class="math inline">\(z=f(x,y)\)</span> 是一个二维曲面， <span class="math inline">\(g(x,y)=c\)</span> 是一个母线平行于Z轴的柱面。拉格朗日乘子基于这样一个想法： <span class="math inline">\(f(x,y)\)</span> 经过最小值点的等高线一定和 <span class="math inline">\(g(x,y)=c\)</span> 相切。如果不相切，说明还存在更低的等高线和柱面存在交点，在交点上的值更小，这就和假设矛盾了。</p>
<blockquote>
<p>也可以这样理解：<span class="math inline">\(f(x,y)\)</span>和<span class="math inline">\(g(x,y)=c\)</span>的交线上的最小值点是<span class="math inline">\((x^*,y^*)\)</span>，那么<span class="math inline">\(\nabla f(x^*,y^*)\)</span>一定垂直于交线，否则沿着交线走，梯度的分量不会是0，那么必然遇到z更小的点。</p>
</blockquote>
<p><img src="LagrangeMultipliers3D.png" width="80%" /></p>
<p>（图片取自<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Wikipedia</a>）</p>
<p>同时，<span class="math inline">\(f\)</span> 和 <span class="math inline">\(g\)</span> 的梯度在最小值点平行，即 <span class="math inline">\(\nabla f \parallel \nabla g\)</span> 。要理解这一点，我们把 <span class="math inline">\(g(x,y)\)</span> 放在 <span class="math inline">\(z=g(x,y)\)</span> 这个曲面中重新考虑。这样 <span class="math inline">\(g(x,y)=c\)</span> 就成了这个曲面在平面 <span class="math inline">\(z=c\)</span> 上的一条等高线。根据<strong>等高线的切向量和梯度向量垂直</strong>以及<strong>平面内垂直于同一直线的两条直线平行</strong>的道理，两个梯度向量显然是平行的。于是我们可以把这个结论写成 <span class="math inline">\(\nabla f = -\lambda \nabla g\)</span> 。</p>
<p>但是，这时候存在第二种可能性：如果 <span class="math inline">\(f\)</span> 本身是水平的，不管 <span class="math inline">\(\nabla g\)</span> 是多少， <span class="math inline">\(\lambda=0\)</span> 直接使上式成立。</p>
<p>这两种情况可以整合到一个式子去：</p>
<p><span class="math display">\[
L(x,y,\lambda) = f(x,y) + \lambda \cdot (g(x,y)-c)
\]</span></p>
<p>这样，前面的条件就可以从偏导数等于0的式子中得出：</p>
<p><span class="math display">\[
\nabla_x L=0 \implies f_x=-\lambda g_x \\
\nabla_y L=0 \implies f_y=-\lambda g_y \\
\nabla_{\lambda} L=0 \implies g(x,y)=c
\]</span></p>
<p>无约束条件的优化一般比有约束优化容易求解。拉格朗日乘子的思路就是把后者转化成前者。</p>
<h3 id="多个约束">多个约束</h3>
<p>有多个约束时，前面说的 <span class="math inline">\(f\)</span> 和 <span class="math inline">\(g\)</span> 梯度平行的性质不复存在。这时候有多个 <span class="math inline">\(g\)</span> ，<span class="math inline">\(f\)</span> 的梯度没法和每个 <span class="math inline">\(g\)</span> 的梯度都平行。但是这时候有一个重要性质：<span class="math inline">\(\nabla f\)</span>可以由各个<span class="math inline">\(\nabla g\)</span>线性表出。首先，用 <span class="math inline">\(\{v_L\}\)</span> 表示使f的值保持在同一个level set的方向向量集合，类似前面提到的“等高线的切向量和梯度向量垂直”，有</p>
<p><span class="math display">\[
(\nabla f)^T\cdot v=0,\ v\in \{v_L\}
\]</span></p>
<p>同样的，用 <span class="math inline">\(\{v_C\}\)</span> 表示可以沿着移动而不违反约束条件（即 <span class="math inline">\(g=c\)</span> ）的方向向量集合，则有</p>
<p><span class="math display">\[
(\nabla g)^T\cdot v=0,\ v\in \{v_C\}
\]</span></p>
<p>现在我们有m个约束 <span class="math inline">\(g_i(\mathbf{x})=c_i,\ i=1,2,\ldots,m\)</span> ，那么每个 <span class="math inline">\(\nabla g_i\)</span> 都定义了一个violation direction （沿着跟这个方向不正交的向量移动都会使约束条件不成立），所有的 <span class="math inline">\(\nabla g_i\)</span> 就张成了一个violation space。一个点 <span class="math inline">\(\mathbf{x}\)</span> 是极值点的必要条件是： <span class="math inline">\(\nabla f\)</span> 在这个violation space中。这样就能保证任何减少f的移动都会使约束不满足。也就是说，下式必须成立：</p>
<p><span class="math display">\[
\nabla f + \sum_{k=1}^m {\lambda_k \nabla g_k} = 0
\]</span></p>
<p>对应的拉格朗日算式就是：</p>
<p><span class="math display">\[
L(\mathbf{x},\mathbf{\lambda}) = f(\mathbf{x})+
\sum_{k=1}^m {\lambda_k g_k(\mathbf{x})}
\]</span></p>
<h3 id="不等式约束">不等式约束</h3>
<p>把上面讲的约束换成不等式形式：</p>
<p><span class="math display">\[
\text{minimize}\ f(\mathbf{x}) \\
\text{s.t.}\ g_i(\mathbf{x})\leq 0,\  i=1,2,\ldots,m
\]</span></p>
<p>为方便起见，令 <span class="math display">\[
\mathbf{g}(\mathbf{x})=\left[g_1(\mathbf{x}),g_2(\mathbf{x}),
\ldots,g_m(\mathbf{x})\right]^T
\]</span></p>
<p>那么约束表达式可以写作 <span class="math display">\[
\mathbf{g}(\mathbf{x}) \preceq \mathbf{0}
\]</span></p>
<blockquote>
<p>如果<span class="math inline">\(g_i(\mathbf{x})\)</span>是凸函数（convex function），那么显然 <span class="math inline">\(\{\mathbf{x} \mid g_i(\mathbf{x}) \leq 0\}\)</span> 是一个凸集（convex set），所以 <span class="math inline">\(\{\mathbf{x} \mid \mathbf{g}(\mathbf{x}) \preceq \mathbf{0}\} = \bigcap_{i} \{\mathbf{x} \mid g_i(\mathbf{x}) \leq 0\}\)</span> 也是一个凸集。</p>
</blockquote>
<p>这时候最小值点的必要条件是，对于拉格朗日函数</p>
<p><span class="math display">\[
L(\mathbf{x},\mathbf{\mu})=f(\mathbf{x})+\mathbf{\mu}^T\mathbf{g}(\mathbf{x})
\]</span></p>
<p>以下几个条件必须成立：</p>
<ol type="1">
<li><span class="math inline">\(\nabla L(\mathbf{x}^*,\mathbf{\mu})=\mathbf{0}\)</span></li>
<li><span class="math inline">\(\mathbf{\mu} \succeq \mathbf{0}\)</span></li>
<li><span class="math inline">\(\mathbf{\mu}^T\mathbf{g}(\mathbf{x})=\mathbf{0}\)</span></li>
</ol>
<p>在这里我们省略了等于0的那部分约束，因为SVM中只会用到不等式约束。</p>
<hr />
<p>下面说明为什么必须满足这3个必要条件：</p>
<h4 id="必要条件1">必要条件(1)</h4>
<p>根据可能的最大值点 <span class="math inline">\(\mathbf{x}^*\)</span> 落在 <span class="math inline">\(\mathbf{g}(\mathbf{x}) \preceq \mathbf{0}\)</span> 定义的可行域的内部或是边界，分两种情况讨论：</p>
<ol type="1">
<li><span class="math inline">\(\mathbf{x}^*\)</span>在<span class="math inline">\(g_i(\mathbf{x}) \lt 0\)</span>的区域内部。这样等价于<span class="math inline">\(g_i\)</span>约束不存在，叫做inactive constraint，这时候我们可以直接把它对应的乘子<span class="math inline">\(\mu_i\)</span>设成0；</li>
<li><span class="math inline">\(\mathbf{x}^*\)</span>在<span class="math inline">\(g_i(\mathbf{x}) \leq 0\)</span>定义的边界上，即<span class="math inline">\(g_i(\mathbf{x}^*) = 0\)</span>，这叫做active constraint。</li>
</ol>
<p>Active constraint这种情况其实就退化成前面一节讲的等式约束，那么“ <span class="math inline">\(\nabla f\)</span> 可以写成 <span class="math inline">\(\nabla g\)</span> 的线性组合”这个结论也适用，所以(1)就成立。</p>
<h4 id="必要条件2">必要条件(2)</h4>
<p>假设 <span class="math inline">\(\mathbf{x^*}\)</span> 是最小值，那么可行域内部的其它点可以表示为：</p>
<p><span class="math display">\[
\mathbf{x} = \mathbf{x^*}+\mathbf{p}
\]</span></p>
<p>泰勒展开后 <span class="math inline">\(\mathbf{g}(\mathbf{x})\)</span> 的一阶近似是：</p>
<p><span class="math display">\[
\mathbf{g}(\mathbf{x}) \approx \underbrace{\mathbf{g}(\mathbf{x}^*)}_{\mathbf{0}} +
\nabla\mathbf{g}(\mathbf{x}^*)^T\cdot\mathbf{p} \preceq \mathbf{0}
\]</span></p>
<p>根据条件(1)，</p>
<p><span class="math display">\[
\nabla f(\mathbf{x})=-\mathbf{\mu}^T\cdot\nabla\mathbf{g}(\mathbf{x})
\]</span></p>
<p>既然是最小值点，那么 <span class="math inline">\(f\)</span> 关于 <span class="math inline">\(\mathbf{p}\)</span> 的方向导数必须大于等于0：</p>
<p><span class="math display">\[
\nabla f(\mathbf{x})\cdot\mathbf{p} =
\mathbf{\mu}^T\underbrace{(
-\frac{\partial\mathbf{g}(\mathbf{x})}{\partial\mathbf{x}^T}
\cdot\mathbf{p})}
_{\succeq\mathbf{0}} \geq 0
\]</span></p>
<p>（这里把 <span class="math inline">\(g\)</span> 的梯度写成这种形式是为了显式表明用的是Numerator Layout，即 <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial\mathbf{x}^T}\)</span> 这个矩阵与 <span class="math inline">\(\mathbf{y}\)</span> 同行数，与 <span class="math inline">\(\mathbf{x}^T\)</span> 同列数）</p>
<p>显然 <span class="math inline">\(\mathbf{\mu}\)</span> 的每个元素都要非负才能保证不等式成立。</p>
<h4 id="必要条件3">必要条件(3)</h4>
<p>前面(1)已经讲过两种情况的讨论：</p>
<ol type="1">
<li>条件 <span class="math inline">\(g_i\)</span> 属于inactive constraint时，我们可以直接把它对应的乘子 <span class="math inline">\(\mu_i\)</span> 设成0；</li>
<li>属于active constraint时，有 <span class="math inline">\(g_i(\mathbf{x^*}) = 0\)</span> 。</li>
</ol>
<p>这两种情况可以统一写成(3)： <span class="math inline">\(\mu_i g_i(\mathbf{x}) = 0\)</span> 这也被叫做<em>互补松弛条件</em>（Complementary Slackness Condition）。</p>
<hr />
<p>回过头看拉格朗日函数的定义。要优化的目标函数是关于 <span class="math inline">\(\mathbf{x}\)</span> 的，所以把 <span class="math inline">\(\mathbf{x}\)</span> 称为primal variables，相应的， <span class="math inline">\(\mathbf{\mu}\)</span> 叫做dual variables。</p>
<p>如果把 <span class="math inline">\(\mathbf{\mu}^T\mathbf{g}\)</span> 看作惩罚项，我们可以把拉格朗日函数看作基于原目标函数的penalized objective。如果有某个约束不满足（ <span class="math inline">\(g_i&gt;0\)</span> ），就会得到一个大于0的惩罚项。如果所有约束都得到满足，那么对于可行域中的所有点拉格朗日函数都给出了原函数的一个lower bound：</p>
<p><span class="math display">\[
\forall \mathbf{x}\in\Omega:
f(\mathbf{x})\geq L(\mathbf{x},\mathbf{\mu})
\]</span></p>
<p>现在定义dual目标函数:</p>
<p><span class="math display">\[
h(\mathbf{\mu}) = \min_{\mathbf{x}\in\mathbb{R}^n} L(\mathbf{x},\mathbf{\mu})
\]</span></p>
<p>注意h是一个凹函数（concave function）。可以把 <span class="math inline">\(h\)</span> 直观理解成双曲抛物面 <span class="math inline">\(z=x^2-y^2\)</span> 被平面 <span class="math inline">\(x=0\)</span> 截得的曲线。</p>
<p>可以推出</p>
<p><span class="math display">\[
L(\mathbf{x},\mathbf{\mu}) \geq h(\mathbf{\mu}) \\
\forall \mathbf{x}\in\Omega,\ \mathbf{\mu}\succeq 0:
f(\mathbf{x})\geq h(\mathbf{\mu})
\]</span></p>
<h3 id="min-max-duality">Min-Max Duality</h3>
<p>假设有两个玩家X和Y玩一个策略游戏，X的目标是让 <span class="math inline">\(F(x,y)\)</span> 尽可能的小， Y刚好相反。类似AI里面的最大值-最小值算法，分析一下他们各自最优的博弈策略：</p>
<p>X：如果我选择x，因为Y是一个理性的玩家，他一定会选择对我最不利的策略（让F最大），即</p>
<p><span class="math display">\[
F^*(x) = \max_y F(x,y)
\]</span></p>
<p>所以我必须选择在所有情况下对我最优的策略，也就是</p>
<p><span class="math display">\[
\min_x F^*(x) \\
= \min_x \max_y F(x,y)
\]</span></p>
<p>从Y的角度来考虑可以得到一个对称的解，二者互为对偶问题：</p>
<p><span class="math display">\[
\max_y F_*(y) \\
= \max_y \min_x F(x,y)
\]</span></p>
<p>P.S. 这种解读方法被称为对偶问题的Game Interpretation。</p>
<h3 id="鞍点条件saddle-point-condition">鞍点条件（Saddle Point Condition）</h3>
<p>鞍点：</p>
<p><span class="math display">\[
L(\bar x, y) \leq L(\bar x, \bar y) \leq L(x, \bar y)
\]</span></p>
<p>把上一节的最大/最小值用上/下界来改写，</p>
<p>下式恒成立：</p>
<p><span class="math display">\[
\sup_y \inf_x L(x,y) \leq \inf_x \sup_y L(x,y)
\]</span></p>
<p>这是因为：设sup-inf对应的坐标点是 <span class="math inline">\((x_*,y_*)\)</span> ，inf-sup对应的是 <span class="math inline">\((x^*,y^*)\)</span> ，那么有</p>
<p><span class="math display">\[
L(x_*,y_*) \leq L(x^*,y_*) \leq L(x^*,y^*)
\]</span></p>
<p>另一方面，如果 <span class="math inline">\((\bar x, \bar y)\)</span> 是L的一个鞍点，那么</p>
<p><span class="math display">\[
\inf_x \sup_y L(x,y) \leq \sup_y L(\bar x,y) \leq L(\bar x, \bar y)
\leq \inf_x L(x, \bar y) \leq \sup_y \inf_x L(x,y)
\]</span></p>
<p>于是</p>
<p><span class="math display">\[
\sup_y \inf_x L(x,y) = \inf_x \sup_y L(x,y)
\]</span></p>
<p>这说明sup和inf的顺序可以交换。</p>
<p>Dual problem给出了primal (minimization) problem的lower bound。</p>
<h2 id="dual">Dual</h2>
<p>在一些情况下，间接地优化对偶函数能得到比原始问题更简单的形式，尤其是当 <span class="math inline">\(\mathbf{x}\)</span> 的维度远远高于约束方程个数的时候。</p>
<p>我们将看到，SVM优化的对偶形式可以让关于 <span class="math inline">\(\mathbf{x}\)</span> 的计算表达成内积的形式。这样就可以把 <span class="math inline">\(\mathbf{x}\)</span> 用“核函数”映射到某个高维空间（比如径向基函数就是无穷维的），而模型的其他部分保持不变，以此免费获得求解非线性问题的能力。</p>
<!-- TODO -->
<h2 id="primal-form">Primal form</h2>
<p>SVM的优化算式是</p>
<p><span class="math display">\[
\min_{w,b} \frac{1}{2}\|\mathbf{w}\|^2 \\
\text{s.t.}\ y_i(\mathbf{w}\cdot\mathbf{x}_i-b) \ge 1
\]</span></p>
<p>引入拉格朗日乘子 <span class="math inline">\(\mathbf{\alpha}\)</span> ，写成</p>
<p><span class="math display">\[
L(\mathbf{w},b;\mathbf{\alpha})=
\frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^n\alpha_i
\left(y_i(\mathbf{w}\cdot\mathbf{x}_i-b)-1\right)
\]</span></p>
<p>当条件 <span class="math inline">\(y_i(\mathbf{w}\cdot\mathbf{x}_i-b) \ge 1\)</span> 不满足时，只要把对应的 <span class="math inline">\(\alpha_i\)</span>设成<span class="math inline">\(+\infty\)</span> 就会让 <span class="math inline">\(L=+\infty\)</span> ；如果条件满足，拉格朗日函数的值就和目标函数一样：</p>
<p><span class="math display">\[
\sup L(\mathbf{w},b;\mathbf{\alpha}) =
\begin{cases}
+\infty &amp;\text{ if } \exists i:y_i(\mathbf{w}\cdot\mathbf{x}_i-b) &lt; 1 \\
\frac{1}{2}\|\mathbf{w}\|^2 &amp;\text{ otherwise }
\end{cases}
\]</span></p>
<p>去优化这个算式，得到</p>
<p><span class="math display">\[
\min_{\mathbf{w},b} \max_{\mathbf{\alpha} \succeq 0}
L(\mathbf{w},b;\mathbf{\alpha})
\]</span></p>
<p>这就是SVM的primal problem。对应的Dual problem是：</p>
<p><span class="math display">\[
\max_{\mathbf{\alpha} \succeq 0} \min_{\mathbf{w},b}
L(\mathbf{w},b;\mathbf{\alpha})
\]</span></p>
<p>接下来的推导只要代入化简即可。如果用爱因斯坦求和约定，可以把计算过程表达得很简洁：</p>
<p>把 <span class="math inline">\(L\)</span> 对 <span class="math inline">\(\mathbf{w}\)</span> 和 <span class="math inline">\(b\)</span> 取偏导数，令其等于0，可得</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \mathbf{w}} &amp;= \mathbf{w} -
  \alpha_i y_i \mathbf{x}_i = 0 \\
&amp; \implies \\
\mathbf{w} &amp;= \alpha_i y_i \mathbf{x}_i
\end{aligned}
\]</span></p>
<p>以及</p>
<p><span class="math display">\[
\frac{\partial L}{\partial b} = \alpha_i y_i = 0
\]</span></p>
<p>把 <span class="math inline">\({\partial L}/{\partial \mathbf{w}} = 0\)</span> 推导出的结果代入优化方程，得</p>
<p><span class="math display">\[
\begin{aligned}
L &amp;= \frac{1}{2} \alpha_i \alpha_j y_i y_j  (\mathbf{x}_i \cdot \mathbf{x}_j) -
    \alpha_i \alpha_j y_i y_j  (\mathbf{x}_i \cdot \mathbf{x}_j) - b\ \alpha_i \mathbf{y}_i + \alpha_i \\
  &amp;= \alpha_i - \frac{1}{2} \alpha_i \alpha_j y_i y_j  (\mathbf{x}_i \cdot \mathbf{x}_j)
\end{aligned}
\]</span></p>
<p>因为 <span class="math inline">\(\alpha\)</span> 是前面引入的拉格朗日乘子，自然有</p>
<p><span class="math display">\[
\alpha_i \geq 0
\]</span></p>
<p>又根据 <span class="math inline">\({\partial L}/{\partial b} = 0\)</span> 的结果知道</p>
<p><span class="math display">\[
\alpha_i y_i = 0
\]</span></p>
<p>从而KKT条件满足，Dual的结果和Primal相等。这一点的重要意义是，因为 <span class="math inline">\(\mathbf{w}\)</span> 可以由少量 <span class="math inline">\(\alpha \neq 0\)</span> 的样本点线性表出，当我们把训练好的模型对新数据做分类的时候，计算过程可以简化为</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{w}\cdot\mathbf{x}_i-b &amp;=
  (\alpha_i y_i \mathbf{x}_i) \cdot \mathbf{x} - b \\
&amp;= \alpha_i y_i (\mathbf{x}_i \cdot \mathbf{x}) - b
\end{aligned}
\]</span></p>
<p>也就是说，只要把 <span class="math inline">\(\mathbf{x}\)</span> 和训练集中几个 <em>支持向量</em> 求内积就可以做出预测。不光计算简单，还为之后使用 kernel trick 扫清了障碍。</p>
<p>（未完待续）</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>Standford CS229</li>
<li>Wikipedia</li>
</ul>

        </div>
        <div id="footer">
            Copyright &copy; 2015 - Yukun Guo -
            Powered by <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js" integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body,);"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-KNSY6VRL5Y"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-KNSY6VRL5Y');
    </script>
    </body>
</html>
